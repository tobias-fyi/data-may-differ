{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-fi IRL: 001\n",
    "\n",
    "### A Data Storytelling Project by Tobias Reaper\n",
    "\n",
    "### ---- Datalogue 001 ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [DataQuest tutorial on using python for large json datasets](https://www.dataquest.io/blog/python-json-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from io import StringIO, BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"edited\":false,\"id\":\"dbumnpz\",\"parent_id\":\"t1_dbulzrw\",\"distinguished\":null,\"created_utc\":1483228800,\"author_flair_text\":null,\"author_flair_css_class\":\"NYAN\",\"controversiality\":0,\"subreddit_id\":\"t5_22i0\",\"retrieved_on\":1485679711,\"link_id\":\"t3_5lc6zb\",\"author\":\"captnkaposzta\",\"score\":2,\"gilded\":0,\"stickied\":false,\"body\":\"Beileid? Kiwi Fernsehgarten Trinkspiele retten jede Party.\\n\\nGesundes Neues\\n\",\"subreddit\":\"de\"}\n",
      "{\"author\":\"CampyJejuni\",\"score\":3,\"stickied\":false,\"gilded\":0,\"body\":\"Wrong subreddit mate.\",\"subreddit\":\"TwoXChromosomes\",\"controversiality\":0,\"author_flair_css_class\":null,\"author_flair_text\":null,\"subreddit_id\":\"t5_2r2jt\",\"retrieved_on\":1485679711,\"link_id\":\"t3_5lai4x\",\"created_utc\":1483228800,\"distinguished\":null,\"parent_id\":\"t1_dbum9w2\",\"edited\":false,\"id\":\"dbumnq0\"}\n",
      "{\"distinguished\":null,\"created_utc\":1483228800,\"edited\":false,\"id\":\"dbumnq1\",\"parent_id\":\"t3_5lb9zs\",\"author\":\"Luigimario280\",\"score\":7,\"stickied\":false,\"body\":\"Karma!\",\"gilded\":0,\"subreddit\":\"BikiniBottomTwitter\",\"author_flair_css_class\":null,\"author_flair_text\":null,\"controversiality\":0,\"subreddit_id\":\"t5_3deqz\",\"retrieved_on\":1485679711,\"link_id\":\"t3_5lb9zs\"}\n",
      "{\"retrieved_on\":1485679711,\"subreddit_id\":\"t5_2sjgc\",\"author_flair_css_class\":null,\"author_flair_text\":null,\"controversiality\":0,\"link_id\":\"t3_5la4p0\",\"score\":1,\"author\":\"MRA-automatron-2kb\",\"subreddit\":\"MGTOW\",\"stickied\":false,\"body\":\"Oh... thanks for answering.\",\"gilded\":0,\"edited\":false,\"id\":\"dbumnq2\",\"parent_id\":\"t1_dbumdpx\",\"distinguished\":null,\"created_utc\":1483228800}\n",
      "{\"link_id\":\"t3_5l9ny9\",\"author_flair_css_class\":null,\"author_flair_text\":null,\"controversiality\":0,\"subreddit_id\":\"t5_2scss\",\"retrieved_on\":1485679711,\"stickied\":false,\"gilded\":0,\"body\":\"Do you buy the minis yourself and include that in your fee or do you get shipped the mini from the client, if so assembled or not?\",\"subreddit\":\"minipainting\",\"author\":\"Godcon\",\"score\":2,\"parent_id\":\"t1_dbukpfs\",\"id\":\"dbumnq3\",\"edited\":false,\"created_utc\":1483228800,\"distinguished\":null}\n",
      "{\"created_utc\":1483228800,\"distinguished\":null,\"parent_id\":\"t3_5lc8bn\",\"edited\":false,\"id\":\"dbumnq4\",\"author\":\"fendaar\",\"score\":1,\"stickied\":false,\"body\":\"Rule 7\",\"gilded\":0,\"subreddit\":\"tipofmytongue\",\"author_flair_css_class\":null,\"controversiality\":0,\"author_flair_text\":null,\"subreddit_id\":\"t5_2r4oc\",\"retrieved_on\":1485679711,\"link_id\":\"t3_5lc8bn\"}\n",
      "{\"stickied\":false,\"gilded\":0,\"body\":\"[deleted]\",\"subreddit\":\"AsiansGoneWild\",\"author\":\"[deleted]\",\"score\":2,\"link_id\":\"t3_5lb7qy\",\"author_flair_css_class\":null,\"controversiality\":0,\"author_flair_text\":null,\"subreddit_id\":\"t5_2wm0g\",\"retrieved_on\":1485679711,\"created_utc\":1483228800,\"distinguished\":null,\"parent_id\":\"t3_5lb7qy\",\"edited\":false,\"id\":\"dbumnq5\"}\n",
      "{\"distinguished\":null,\"created_utc\":1483228800,\"id\":\"dbumnq6\",\"edited\":false,\"parent_id\":\"t1_dbuksbh\",\"subreddit\":\"gis\",\"body\":\"Can I ask where you work now and what you do, and if you enjoy it?\",\"stickied\":false,\"gilded\":0,\"score\":1,\"author\":\"Fleetthrow\",\"link_id\":\"t3_5l0tz4\",\"subreddit_id\":\"t5_2qmpb\",\"retrieved_on\":1485679711,\"author_flair_text\":null,\"author_flair_css_class\":null,\"controversiality\":0}\n",
      "{\"subreddit\":\"SeattleWA\",\"body\":\"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\",\"stickied\":false,\"gilded\":0,\"score\":1,\"author\":\"music4mic\",\"link_id\":\"t3_5l79wh\",\"retrieved_on\":1485679711,\"subreddit_id\":\"t5_2vbli\",\"controversiality\":0,\"author_flair_css_class\":null,\"author_flair_text\":null,\"created_utc\":1483228800,\"distinguished\":null,\"parent_id\":\"t1_dbts1i6\",\"id\":\"dbumnq7\",\"edited\":false}\n",
      "{\"distinguished\":null,\"created_utc\":1483228800,\"edited\":false,\"id\":\"dbumnq8\",\"parent_id\":\"t1_dbu5bpp\",\"subreddit\":\"Games\",\"gilded\":0,\"stickied\":false,\"body\":\"Its truly a great experience running 3 tank lineups then one player tilting mid game and picking torb.\",\"score\":1,\"author\":\"FatalTouch\",\"link_id\":\"t3_5l954r\",\"subreddit_id\":\"t5_2qhwp\",\"retrieved_on\":1485679711,\"controversiality\":0,\"author_flair_css_class\":null,\"author_flair_text\":null}\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "head RC_2017-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like the data is oriented in the \"records\" format\n",
    "\n",
    "# reader is an iterator that returns \"chunksize\" lines each iteration\n",
    "reader = pd.read_json(\"RC_2017-01.bz2\", orient=\"records\", lines=True, chunksize=1000, compression=\"bz2\")\n",
    "\n",
    "# TODO: add column conversions\n",
    "#         - to datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.json._json.JsonReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each chunk of the reader is a DataFrame\n",
    "# Meaning it can be filtered\n",
    "for chunk in reader:\n",
    "    print(chunk[chunk[\"subreddit\"] == \"sciencefiction\"])\n",
    "    \n",
    "# TODO: create list of subreddits to filter on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushshift Reddit API Documentation\n",
    "\n",
    "This RESTful API gives full functionality for searching Reddit data and also includes the capability of creating powerful data aggregations.  With this API, you can quickly find the data that you are interested in and find fascinating correlations.  \n",
    "\n",
    "### Understanding the API\n",
    "\n",
    "There are two main ways of accessing the Reddit comment and submission database.  One is by using the API directly via https://api.pushshift.io/ and the other is through accessing the back-end Elasticsearch search engine via https://elastic.pushshift.io/  This document will explain both approaches and give examples on how to effectively use the API.  This document will also explore the use of the API parameters to utilize more focused searches.\n",
    "\n",
    "### Using the https://api.pushshift.io endpoints\n",
    "\n",
    "There are two main endpoints used to search all publicly available comments and submissions on Reddit:\n",
    "\n",
    "* /reddit/search/comment\n",
    "* /reddit/search/submission\n",
    "\n",
    "In the next section, we will explore how to perform more effective searches using the comment search endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Comments\n",
    "\n",
    "To search for comments, use the https://api.pushshift.io/reddit/search/comment/ endpoint.  Let's start with a few examples and then go over the various parameters available when using this endpoint.  One of the simplest searches is using just the q parameter.  The q parameter is used to search for a specific word or phrase.  Here is an example:\n",
    "\n",
    "**Search for the most recent comments mentioning the word \"science\"**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=science\n",
    "\n",
    "This will search the most recent comments with the term \"science\" in the body of the comment.  This search is not case-sensitive, so it will find any occurence of the term \"science\" regardless of capitalization.  The API defaults to sorting by recently made comments first.  After performing this search, 25 results are returned.  This is the default size for searches and can be adjusted using the size parameter.  This will be discussed in further detail in the parameters section.  Data is returned in JSON format and actual search results are included in the \"data\" key.  There is also a \"metadata\" key that gives additional information about the search including total number of results found, how long the search took to process, etc.  If aggregations are requested, all aggregation data is returned under the aggs key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search parameters for comments\n",
    "\n",
    "There are numerous additional parameters that can be used when performing a comment search.  Let's go over them and provide examples for each.\n",
    "\n",
    "| Parameter | Description | Default | Accepted Values | \n",
    "| ------ | ------ | ------- | ------ |\n",
    "| q | Search term. | N/A | String / Quoted String for phrases |\n",
    "| ids | Get specific comments via their ids | N/A | Comma-delimited base36 ids |\n",
    "| size | Number of results to return | 25 | Integer <= 500\n",
    "| fields | One return specific fields (comma delimited) | All Fields Returned | string or comma-delimited string\n",
    "| sort | Sort results in a specific order | \"desc\" | \"asc\", \"desc\"\n",
    "| sort_type | Sort by a specific attribute | \"created_utc\" | \"score\", \"num_comments\", \"created_utc\"\n",
    "| aggs | Return aggregation summary | N/A | [\"author\", \"link_id\", \"created_utc\", \"subreddit\"]\n",
    "| author | Restrict to a specific author | N/A | String\n",
    "| subreddit | Restrict to a specific subreddit | N/A | String\n",
    "| after | Return results after this date | N/A | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days)\n",
    "| before | Return results before this date | N/A | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days)\n",
    "| frequency | Used with the aggs parameter when set to created_utc | N/A | \"second\", \"minute\", \"hour\", \"day\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting comments based on id\n",
    "\n",
    "You can retrieve comments directly by using the ids parameter.  To get a batch of comments by their id, use the following example:\n",
    "\n",
    "**Retrieve three comments using their base 36 id values**\n",
    "\n",
    "https://api.pushshift.io/reddit/comment/search?ids=dlrezc8,dlrawgw,dlrhbkq\n",
    "\n",
    "### Using the subreddit parameter\n",
    "\n",
    "There are quite a few parameters to review, so let's start by providing some more complex examples and how to use the parameters above.  Let's continue with the previous example above and expand on our \"science\" keyword search.  What if we wanted to search for the term \"science\" but restrict it to a specific subreddit?  By using the subreddit parameter, we can do that:\n",
    "\n",
    "**Search for the most recent comments mentioning the word \"science\" within the subreddit /r/askscience**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=science&subreddit=askscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the sort and size parameters\n",
    "\n",
    "This will return 25 comments containing the term \"science\" but only from the /r/askscience subreddit.  Since we didn't ask for a specific sort method, the most recent comments are returned (the sort parameter defaults to \"desc\").  What if we wanted the first comment ever to /r/askscience that mentioned the word \"science\"?  We could use the sort and size parameters to handle that.\n",
    "\n",
    "**Search for the most recent comments mentioning the word \"science\" within the subreddit /r/askscience**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=science&subreddit=askscience&sort=asc&size=1\n",
    "\n",
    "This is the result:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"author\": \"MockDeath\",\n",
    "            \"author_flair_css_class\": null,\n",
    "            \"author_flair_text\": null,\n",
    "            \"body\": \"Knowing more would definitely help.  I guess all you can do is find out if they know the basics like you said then take it from there.  That CO\\u00b2 has the carbon turned to the isotope carbon14 in the upper atmosphere by cosmic radiation.  This causes a specific percentage of carbon in the atmosphere to be carbon14.\\n\\nNow we are carbon based life forms and we have to get the carbon we are built out of from some where.  We get it from eating plants, and the plants get it from absorbing CO\\u00b2 from the air.  So so long as we are alive, we uptake new carbon14.  So this gives you a pretty good base line for dating.\\n\\nNow to fight arguments against carbon dating you could use the example of how we can see proton collisions in the LHC for sensitivity of our equipment.  Nuclear decay is very accurate in how fast it happens, this is why atomic clocks work to a much higher degree of accuracy than other methods of time keeping.  Also, you might want to make a general appeal for science.  Science works, that is why we have TV's, robots, particle accelerators, satellites, computers, MRI and CAT scanners, nuclear power, etc etc.  Scientists are not just willy nilly making shit up, or these kinds of things wouldn't work.\",\n",
    "            \"created_utc\": 1270637661,\n",
    "            \"id\": \"c0nn9iq\",\n",
    "            \"link_id\": \"t3_bne3u\",\n",
    "            \"parent_id\": \"t1_c0nn5ux\",\n",
    "            \"score\": 2,\n",
    "            \"subreddit\": \"askscience\",\n",
    "            \"subreddit_id\": \"t5_2qm4e\"\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"execution_time_milliseconds\": 30.52,\n",
    "        \"results_returned\": 1,\n",
    "        \"shards\": {\n",
    "            \"failed\": 0,\n",
    "            \"successful\": 36,\n",
    "            \"total\": 36\n",
    "        },\n",
    "        \"size\": 1,\n",
    "        \"sort\": \"asc\",\n",
    "        \"sort_type\": \"created_utc\",\n",
    "        \"timed_out\": false,\n",
    "        \"total_results\": 134785,\n",
    "        \"version\": \"v3.0\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "From the result returned, we can see that the first comment ever made to /r/askscience mentioning \"science\" happened on epoch date 1270637661, which translates to Wednesday, April 7, 2010 10:54:21 AM (GMT).  Let's quickly go over the metadata pieces.  We can see that the execution time for this search was around 30 milliseconds.  There were a total of 36 shards searched and all were successful.  The search did not time out (timed_out parameter) which is good.  This is an attribute you may want to check if you use the API programmatically as some searches that are more complicated may sometimes time out.  The total_results value is 134,785.  This tells us the total number of comments in /r/askscience that mention the word science.  Since we did not use the before or after parameters, this number represents the entirety of the comments made to /r/askscience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the before and after parameters \n",
    "\n",
    "Let's continue by using additional parameters to highlight the power of the search API.  The before and after parameters allow you to restrict the time-frame for the search by giving an epoch timestamp for both.  However, the API also understands more human-like values for the before and after parameters.  You can use a number followed by the characters s,m,h,d (which stand for second, minute, hour and day) to limit the time-frame as well.  Let's run through some examples.\n",
    "\n",
    "If you wanted to do a search for \"Rome\" in the subreddit /r/askhistorians but limit it only to the past 30 days, you could use the after parameter with the value 30d (30 days).  \n",
    "\n",
    "**Search the subreddit /r/askhistorians for comments mentioning Rome within the past 30 days**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=rome&subreddit=askhistorians&after=30d\n",
    "\n",
    "What if there was a recent news story three days ago, but we wanted to limit the search window between 4 days ago and 2 days ago?  We could use both the before and after parameter to do so.  In the next example, we will search for comments mentioning Trump that were made between 4 and 2 days ago and sort by ascending.\n",
    "\n",
    "**Search all subreddits for the term \"Trump\" and return comments made between 2 and 4 days ago**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=4d&before=2d&sort=asc\n",
    "\n",
    "### Using the fields parameter\n",
    "\n",
    "Let's say you wanted to do a search for the last 150 comments, but you only need the author and body fields returned for each comment.  Using the fields parameter, you can tell the API which pieces of information you want to filter.  This is primarily to help reduce bandwidth if you are making a lot of requests and only need specific fields returned.\n",
    "\n",
    "Here is an example using the fields parameter to search for the past 150 comments that mention \"government\" and only returning the author and body fields:\n",
    "\n",
    "**Search all subreddits for the term \"government\" and return comments with only the body and author keys**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=government&size=150&fields=body,author\n",
    "\n",
    "### Using the author parameter\n",
    "\n",
    "Using one of the examples above that searched for the first occurrence of the word \"science\" in the subreddit /r/askscience, we saw that the author of the comment was \"MockDeath.\"  What if we wanted to get the first 100 comments that \"MockDeath\" made to Reddit?  We can use the author parameter, along with the sort and size parameters.\n",
    "\n",
    "**Search all subreddits and get the first 100 comments ever made by the user /u/MockDeath**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?author=MockDeath&sort=asc&size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the aggs parameter\n",
    "\n",
    "Aggregations is a powerful method to give summary data for a search.  Using the aggs parameter, we can quickly create facets around specific parameters and see how data changes over time.  The aggs parameter for comment searches accepts the following values:  author, subreddit, reated_utc and link_id.  We can do a lot of very cool things using this parameter, so let's dive into some examples.\n",
    "\n",
    "### Using the time frequency (created_utc) aggregation\n",
    "\n",
    "Let's say we wanted to see the frequency of usage for the term \"Trump\" over time.  We'd like to be able to see how many comments were posted per hour over the past 7 days for this term.  Using aggregations and the aggs parameter, we can get that data quickly.  Here's an example using this criteria:\n",
    "\n",
    "**Create a time aggregation using the term trump to show the number of comments mentioning trump each hour over the past 7 days**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=7d&aggs=created_utc&frequency=hour&size=0\n",
    "\n",
    "We used the frequency parameter along with the aggs parameter to create hourly buckets to show the total number of comments mentioning Trump over the past 7 days.  The size parameter was set to 0 because we are only interested in getting aggregation data and not comment data.  The aggregation data is returned in the response under the key aggs -> created_utc.  Here is a snippet of the first part of the return:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"aggs\": {\n",
    "        \"created_utc\": [\n",
    "            {\n",
    "                \"doc_count\": 685,\n",
    "                \"key\": 1502406000\n",
    "            },\n",
    "            {\n",
    "                \"doc_count\": 1238,\n",
    "                \"key\": 1502409600\n",
    "            },\n",
    "            {\n",
    "                \"doc_count\": 1100,\n",
    "                \"key\": 1502413200\n",
    "            },\n",
    "```\n",
    "\n",
    "The doc_count value is the total number of comments containing the term \"trump.\"  The key value is the epoch time for that particular bucket.  In this example, the first bucket has an epoch time of 1502406000 which corresponds to Thursday, August 10, 2017 11:00:00 PM.  This key value is the beginning time of the bucket, so in this example, 685 comments contain the term \"trump\" between the time Thursday, August 10, 2017 11:00:00 PM and Thursday, August 10, 2017 12:00:00 PM.  The frequency parameter allows you to create buckets per second, minute, hour, day, week, month, year.  Using this aggregation, you could use the data to create a chart (i.e. Highcharts) and graph the activity of comments for specific terms, authors, subreddits, etc.  This is an extremely powerful data analysis tool.\n",
    "\n",
    "### Using the subreddit aggregation\n",
    "\n",
    "What if you wanted to not only get the frequency of specific comment terms over time, but also wanted to see which subreddits were the most popular for a given term over that time period?  Here's an example of using the aggs parameters to show which subreddits had the most activity for a specific term.\n",
    "\n",
    "**Create a subreddit aggregation using the term trump to show the top subreddits mentioning trump over the past 7 days**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=7d&aggs=subreddit&size=0\n",
    "\n",
    "Here is a snippet of the result:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"aggs\": {\n",
    "        \"subreddit\": [\n",
    "            {\n",
    "                \"bg_count\": 66,\n",
    "                \"doc_count\": 44,\n",
    "                \"key\": \"lovetrumpshaters\",\n",
    "                \"score\": 0.6666666666666666\n",
    "            },\n",
    "            {\n",
    "                \"bg_count\": 20,\n",
    "                \"doc_count\": 9,\n",
    "                \"key\": \"Denmark_Uncensored\",\n",
    "                \"score\": 0.45\n",
    "            },\n",
    "            {\n",
    "                \"bg_count\": 51,\n",
    "                \"doc_count\": 16,\n",
    "                \"key\": \"WhoRedditHatesNow\",\n",
    "                \"score\": 0.3137254901960784\n",
    "            },\n",
    "```\n",
    "\n",
    "The subreddit aggregation will return the total number of comments in that subreddit that mention the query term (doc_count) as well as the total number of comments made to that subreddit during that time period (bg_count).  This not only will show you which subreddits mentioned Trump the most often, but it also gives you normalized results so that you can also see what percentage of that subreddit's comments contained the search term.  If you were to simply rank the subreddits by which subreddits mentioned the search term \"trump\" the most often, the results would be biased towards subreddits that also contain the most activity in general.  Using this approach, you can see both the raw count and also the normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the submission (link_id) aggregation\n",
    "\n",
    "The API also allows aggregations on link_id, which is another very powerful method to see which submissions are the most popular based on a specific search term.  Continuing with the examples above, let's give a scenario where this would be extremely helpful.  Within the past 24 hours, numerous big stories have dropped concerning Donald Trump.  You would like to use the API to see which submissions are related to Trump based on the number of comments mentioning him within the submissions.  We can again use the aggs parameter and set it to link_id to get this information quickly.  Let's proceed with another example:\n",
    "\n",
    "**Show submissions made within the past 24 hours that mention trump often in the comments**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=24h&aggs=link_id&size=0\n",
    "\n",
    "This will return under the aggs -> link_id key an array of submission objects.  The doc_count gives the total number of comments for each submission that mention the search term (\"trump\") and the bg_count give the total number of comments made to that submission.  This is a great way to quickly find submissions that are \"hot\" based on a specific search term or phrase. \n",
    "\n",
    "### Using the author aggregation\n",
    "\n",
    "The API also allows you to create aggregations on authors so you can quickly see which authors make the most comments for a specific search term. Here is an example of using the author aggregation:\n",
    "\n",
    "**Show the top authors mentioning the term \"Trump\" over the past 24 hours**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=24h&aggs=author&size=0\n",
    "\n",
    "```\n",
    "{\n",
    "    \"aggs\": {\n",
    "        \"author\": [\n",
    "            {\n",
    "                \"doc_count\": 605,\n",
    "                \"key\": \"grrrrreat\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_count\": 329,\n",
    "                \"key\": \"AutoModerator\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_count\": 168,\n",
    "                \"key\": \"autotldr\"\n",
    "            },\n",
    "            {\n",
    "                \"doc_count\": 73,\n",
    "                \"key\": \"SnapshillBot\"\n",
    "            },\n",
    "```\n",
    "\n",
    "The author aggregation will show you which authors make the most comments containing a specific query term.  From the example above, a lot of the top authors mentioning the term \"Trump\" are actually bots.\n",
    "\n",
    "### Combining multiple aggregations at once\n",
    "\n",
    "Using the aggs parameter, you can combine multiple aggregations and get a lot of facet data for a specific term.  Using the examples above, we can combine all of the calls into one call and show the top submissions over the past 24 hours, the frequency of comments per hour mentioning Trump, the top authors posting about Trump and the top subreddits that have had comments made mentioning Trump.\n",
    "\n",
    "**Show aggregations for authors, submissions, subreddits and time frequency for the term \"Trump\" over the past 24 hours**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/comment/?q=trump&after=24h&aggs=author,link_id,subreddit,created_utc&frequency=hour&size=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Searching Submissions\n",
    "\n",
    "To search for submissions, use the endpoint https://api.pushshift.io/reddit/search/submission/ endpoint.  Let's start with a few examples and then go over the various parameters available when using this endpoint.  Do to a simple search, the q parameter is used to search for a specific word or phrase.  Here is an example:\n",
    "\n",
    "**Search for the most recent submissions mentioning the word \"science\"**\n",
    "\n",
    "https://api.pushshift.io/reddit/search/submission/?q=science\n",
    "\n",
    "This will search for the most recent submissions with the word science in the title or selftext.  The search is not case-sensitive, so it will find any occurence of science regardless of capitalization.  The API defaults to sorting by the most recently made submissions first.  After running this search, 25 results are returned.  This is the default size for searches and can be changed by using the size parameter.  This will be discussed in further detail in the parameters section.  Data is returned in JSON format and results are included in the \"data\" key. \n",
    "\n",
    "### Search parameters for submissions\n",
    "\n",
    "There are numerous additional parameters that can be used when performing a submission search.  Let's go over each of them now and provide examples for each one.\n",
    "\n",
    "| Parameter | Description | Default | Accepted Values | \n",
    "| ------ | ------ | ------- | ------ |\n",
    "| ids | Get specific submissions via their ids | N/A | Comma-delimited base36 ids |\n",
    "| q | Search term. Will search ALL possible fields | N/A | String / Quoted String for phrases |\n",
    "| q:not | Exclude search term.  Will exclude these terms | N/A | String / Quoted String for phrases |\n",
    "| title | Searches the title field only | N/A | String / Quoted String for phrases |\n",
    "| title:not | Exclude search term from title.  Will exclude these terms | N/A | String / Quoted String for phrases |\n",
    "| selftext | Searches the selftext field only | N/A | String / Quoted String for phrases |\n",
    "| selftext:not | Exclude search term from selftext.  Will exclude these terms | N/A | String / Quoted String for phrases |\n",
    "| size | Number of results to return | 25 | Integer <= 500 |\n",
    "| fields | One return specific fields (comma delimited) | All Fields | String or comma-delimited string (Multiple values allowed) |\n",
    "| sort | Sort results in a specific order | \"desc\" | \"asc\", \"desc\"\n",
    "| sort_type | Sort by a specific attribute | \"created_utc\" | \"score\", \"num_comments\", \"created_utc\"\n",
    "| aggs | Return aggregation summary | N/A | [\"author\", \"link_id\", \"created_utc\", \"subreddit\"]\n",
    "| author | Restrict to a specific author | N/A | String or comma-delimited string (Multiple values allowed) |\n",
    "| subreddit | Restrict to a specific subreddit | N/A | String or comma-delimited string (Multiple values allowed) |\n",
    "| after | Return results after this date | N/A | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days) |\n",
    "| before | Return results before this date | N/A | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days) |\n",
    "| score | Restrict results based on score | N/A | Integer or > x or < x (i.e. score=>100 or score=<25) |\n",
    "| num_comments | Restrict results based on number of comments | N/A | Integer or > x or < x (i.e. num_comments=>100) |\n",
    "| over_18 | Restrict to nsfw or sfw content | both allowed | \"true\" or \"false\" |\n",
    "| is_video | Restrict to video content | both allowed | \"true\" or \"false\" |\n",
    "| locked | Return locked or unlocked threads only | both allowed | \"true\" or \"false\" |\n",
    "| stickied | Return stickied or unstickied content only | both allowed | \"true\" or \"false\" |\n",
    "| spoiler | Exclude or include spoilers only | both allowed | \"true\" or \"false\" |\n",
    "| contest_mode | Exclude or include content mode submissions | both allowed | \"true\" or \"false\" | \n",
    "| frequency | Used with the aggs parameter when set to created_utc | N/A | \"second\", \"minute\", \"hour\", \"day\" | \n",
    "\n",
    "### Get all comment ids for a particular submission\n",
    "\n",
    "This call is very helpful when used along with Reddit's API. When there are large submissions with thousands of comments, it is often difficult to get all the comment ids for a submission.  This call will return an array of comment ids when a submission id is passed to it.  The endpoint is: https://api.pushshift.io/reddit/submission/comment_ids/{base36 submission id}\n",
    "\n",
    "This call will return a data key with an array of comment ids.  You can then retrieve the actual comment information from this API or the Reddit API.  If the submission is fairly new, it is better to use the Reddit API to get the most current score for the comments.\n",
    "\n",
    "**Retrieve all comment ids for a submission object**\n",
    "\n",
    "https://api.pushshift.io/reddit/submission/comment_ids/6uey5x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## List of Endpoints\n",
    "\n",
    "| Endpoint | Description | Status | \n",
    "| ------ | ------ | ------- |\n",
    "| /reddit/search/comment/ | Search Reddit Comments | Active\n",
    "| /reddit/search/submission/ | Search Reddit Submissions | Active\n",
    "| /reddit/submission/comment_ids/{base36-submission-id} | Retrieve comment ids for a submission object | Active\n",
    "| /reddit/analyze/user/{author-name} | Analyze a Reddit user's activity | In Development\n",
    "| /reddit/term/frequency/{term} | Analyze a term based on activity |  In Development\n",
    "| /reddit/search/all/ | Search Both Comment and Submissions | In Development\n",
    "| /reddit/trending/people | Find out who is trending on Reddit | In Development\n",
    "| /reddit/search/links | Find relevent links being shared on Reddit | In Development\n",
    "\n",
    "\n",
    "> To be continued (Currently under active development) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this link: https://www.reddit.com/r/pushshift/comments/ajmcc0/information_and_code_examples_on_how_to_use_the/ef012vk/\n",
    "\n",
    "# with open(\"filename.zst\", 'rb') as fh:\n",
    "#     dctx = zstd.ZstdDecompressor()\n",
    "#     with dctx.stream_reader(fh) as reader:\n",
    "#         previous_line = \"\"\n",
    "#         while True:\n",
    "#             chunk = reader.read(65536)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "\n",
    "#             string_data = chunk.decode('utf-8')\n",
    "#             lines = string_data.split(\"\\n\")\n",
    "#             for i, line in enumerate(lines[:-1]):\n",
    "#                 if i == 0:\n",
    "#                     line = previous_line + line\n",
    "#                 object = json.loads(line)\n",
    "#                 # do something with the object here\n",
    "#             previous_line = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Key must be string!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-13e3e8650d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RC_2017-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/bigjson/obj.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GET_METHOD_RAISE_EXCEPTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/bigjson/obj.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, key, default, method)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Key must be string!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# TODO: Use some kind of lookup table!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Key must be string!"
     ]
    }
   ],
   "source": [
    "with open(\"RC_2017-01\", \"rb\") as f:\n",
    "    j = bigjson.load(f)\n",
    "    element = j[0]\n",
    "    print(element[\"type\"])\n",
    "    print(element[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas .read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.json._json.JsonReader at 0x124107518>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pd.read_json to...\n",
    "# - manageable chunks\n",
    "# - decompress from .bz2 on the fly\n",
    "\n",
    "# reader is an iterator that returns \"chunksize\" lines each iteration\n",
    "reader = pd.read_json(StringIO(\"RC_2017-01.bz2\"), lines=True, chunksize=1000, compression=\"infer\")\n",
    "\n",
    "reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-56965b52caee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iterate over reader to read in a chunk or two...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mlines_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# Make sure that the returned objects have the right index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/dasci/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Iterate over reader to read in a chunk or two...\n",
    "\n",
    "# Throws an error because the StringIO is considering the filename itself as the string (I think).\n",
    "for chunk in reader:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
